<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CFDfield</title>
  
  
  <link href="https://dr-sunky.github.io/atom.xml" rel="self"/>
  
  <link href="https://dr-sunky.github.io/"/>
  <updated>2024-05-15T20:36:38.089Z</updated>
  <id>https://dr-sunky.github.io/</id>
  
  <author>
    <name>Tian</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OpenFOAM_Workshop_19th</title>
    <link href="https://dr-sunky.github.io/2024/05/14/3-OpenFOAM-Workshop/"/>
    <id>https://dr-sunky.github.io/2024/05/14/3-OpenFOAM-Workshop/</id>
    <published>2024-05-14T22:35:56.000Z</published>
    <updated>2024-05-15T20:36:38.089Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/05/14/3-OpenFOAM-Workshop/velocity.gif" class title="perpendicular flap (velocity field)"><img src="/2024/05/14/3-OpenFOAM-Workshop/overset_mesh_zoom.gif" class title="zoomed view mesh around the tip of flap modified"><img src="/2024/05/14/3-OpenFOAM-Workshop/vorticity_linear.gif" class title="VIV of rigid cyliner (Linear, vorticity)"><img src="/2024/05/14/3-OpenFOAM-Workshop/vorticity_non.gif" class title="VIV of rigid cyliner (NonLinear, vorticity)">]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;/2024/05/14/3-OpenFOAM-Workshop/velocity.gif&quot; class title=&quot;perpendicular flap (velocity field)&quot;&gt;
&lt;img src=&quot;/2024/05/14/3-OpenFOAM-</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Foundamental Theorems of Vector Caculus</title>
    <link href="https://dr-sunky.github.io/2024/05/14/2-Foundamental-Theorems-of-Vector-Caculus/"/>
    <id>https://dr-sunky.github.io/2024/05/14/2-Foundamental-Theorems-of-Vector-Caculus/</id>
    <published>2024-05-14T22:32:53.000Z</published>
    <updated>2024-05-14T22:33:54.109Z</updated>
    
    <content type="html"><![CDATA[<h1 id="fundamental-theorems">Fundamental Theorems</h1><h2 id="gradient-theorem">Gradient Theorem</h2><h2 id="greens-theorem">Green's Theorem</h2><h2 id="stokes-theorem">Stoke's Theorem</h2><h2 id="divergence-theorem-or-gauss-theorem">Divergence Theorem or Gauss Theorem</h2><ul><li><p>In order to transform an equation from the differential to the integral form (vice versa), the Gauss Theorem should be applied.It's important for fluid dynamics.</p></li><li><p><span class="math inline">\(\boldsymbol{V}\)</span> represents a volume in three-dimensional space of boundary <span class="math inline">\(\boldsymbol{S}\)</span>, <span class="math inline">\(\mathbf{n}\)</span> is the outward pointing unit vector normal to <span class="math inline">\(\boldsymbol{S}\)</span>. If <span class="math inline">\(\mathbf{v}\)</span> is a vector field defined on <span class="math inline">\(\boldsymbol{V}\)</span>, then the divergence theorem states that</p></li></ul><p><span class="math display">\[\oint_{\boldsymbol{S}} \mathbf{v} \bullet \mathbf{n} \mathrm{d} S=\int_{\boldsymbol{V}}(\nabla \bullet \mathbf{v}) \mathrm{d} V \]</span></p><ul><li>Implying that the net flux of a vector field through a closed surface is equal to the total volume of all sources and sinks (i.e., the volume integral of its divergence) over the region inside the surface.</li></ul><h2 id="reynolds-transport-theorem">Reynolds Transport Theorem</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;fundamental-theorems&quot;&gt;Fundamental Theorems&lt;/h1&gt;
&lt;h2 id=&quot;gradient-theorem&quot;&gt;Gradient Theorem&lt;/h2&gt;
&lt;h2 id=&quot;greens-theorem&quot;&gt;Green&#39;s Theo</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>CFD Mathematics</title>
    <link href="https://dr-sunky.github.io/2024/05/14/1-CFD-mathematics/"/>
    <id>https://dr-sunky.github.io/2024/05/14/1-CFD-mathematics/</id>
    <published>2024-05-14T22:05:45.000Z</published>
    <updated>2024-05-14T22:34:03.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="basic-mathematics">Basic Mathematics</h1><h2 id="einsteins-summation-convention-爱因斯坦求和约定">Einsteins Summation Convention (爱因斯坦求和约定)</h2><p>For vector and tensor equations, the longest but clearest notation is the Cartesian one. If the equation contains several similar terms which can be summed up , this notation can be abbreviated by applying the Einsteins summation convention.</p><p>对于向量和张量方程，最清晰且最长的表示方法就是笛卡尔表示。但是，如果这些方程中的某些项重复出现，则可以用Einsteins Summation Convention 来简化方程。</p><ul><li>Cartesian form<ul><li><p>The Cartesian form is given by: <span class="math display">\[\frac{\partial \phi_x}{\partial x} + \frac{\partial \phi_y}{\partial y} +\frac{\partial \phi_z}{\partial z}\]</span></p><p>To simply this equation, the Einsteins summation convention can be applied. Commonly, the summation sign <span class="math inline">\(\sum\)</span> can be neglected to keep things clear:</p><p><span class="math display">\[\sum_{i=1} \frac{\partial \phi_i}{\partial x_i} = \frac{\partial\phi_i}{\partial x_i} , i = x,y,z\]</span></p><p>For example: <span class="math display">\[\sum_{i=1}^3 u_i u_i = u_i u_i = u_1u_1 + u_2u_2 + u_3u_3\]</span></p></li></ul></li><li><p>The rules of Einsteins Summation Convention</p><ul><li><p>If the subscript appears more than once in the same term, that means summation operation, and these subscripts are called dummy index</p><p>凡是下脚标重复出现的变量，则认定是求和运算，此时的下脚标被称为“哑标(dummy index)”</p><ul><li><p>the dot production of two vectors 两个速度向量相乘 <span class="math display">\[  u_i u_i = u_1u_1 + u_2u_2 + u_3u_3  \]</span></p></li><li><p>the derivative of velocity 对于速度向量求导 <span class="math display">\[  \frac{d u_i}{d x_i} = \frac{u_1}{x_1} + \frac{u_2}{x_2} + \frac{u_3}{x_3}  \]</span></p></li></ul></li><li><p>If there are two different subscript in one term, for the term which repeat appears, means summation operation, while for the subscript which appears once, traverse is performed for every axis.</p>如果一项中出现两个不同的脚标，对于重复出现的脚标做求和运算，对于单独出现的脚标不作求和运算，而是遍历每一个坐标轴，且每次只遍历一个，单独出现的脚标是自由自在的，称为自由标(free index)“”<ul><li><p>Exp1 例子1 <span class="math display">\[  u_j \frac{d u_i}{d x_i} = u_j \frac{d u_1}{d x_1} + u_j \frac{d u_2}{d x_2} + u_j \frac{d u_3}{d x_3} , (j = 1,2,3)  \]</span></p></li><li><p>Exp2 例子2 <span class="math display">\[u_j \frac{d u_i}{d x_j} = u_1 \frac{d u_i}{d x_1} + u_2 \frac{d u_i}{d x_2} + u_3 \frac{d u_i}{x_3}, (i = 1,2,3)\]</span></p></li></ul></li><li><p>以N-S方程为例 Taking N-S equation for an example $$ \begin{cases}  + u + v + w = f_x -   + ( +  + ) \</p><p> + u + v + w = f_y -   + ( +  + ) \</p><p> + u + v + w = f_z -   + ( +  + ) \end{cases} $$</p><p>Every equations contain <span class="math inline">\(x,y,z\)</span> and <span class="math inline">\(u,v,w\)</span>, they are kind of notations and can be replaced by any other symbols. For example, <span class="math inline">\(x_1, x_2, x_3\)</span> and <span class="math inline">\(u_1, u_2, u_3\)</span> can be used to replace <span class="math inline">\(x,y,z\)</span> and <span class="math inline">\(u,v,w\)</span> respectively.</p><p>Based on the discussion above, the N-S equation can be re-written in tensor form:</p><p><span class="math display">\[\frac{\partial u_i}{\partial t} + u_j \frac{\partial u_i}{\partial x_j} = f_i - \frac{1}{\rho} \frac{\partial p}{\partial x_i} + \frac{\mu}{\rho} \frac{\partial^2 u_i}{\partial x^2_j}\]</span></p></li></ul></li></ul><h2 id="the-classification-of-concept----dot-product-inner-product-outer-product-cross-product">The classification of concept -- dot product, inner product, outer product, cross product</h2><ul><li><p>vector <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> <span class="math display">\[  \boldsymbol{a}=\left(\begin{array}{l}  a_x \\  a_y \\  a_z  \end{array}\right)=\left(\begin{array}{l}  a_1 \\  a_2 \\  a_3  \end{array}\right), \quad \mathbf{b}=\left(\begin{array}{l}  b_x \\  b_y \\  b_z  \end{array}\right)=\left(\begin{array}{l}  b_1 \\  b_2 \\  b_3  \end{array}\right)  \]</span></p></li><li><p>tensor <span class="math inline">\(\mathbf{T}\)</span> <span class="math display">\[  \mathbf{T} =   \left( \begin{array}{cccc}      T_{11} &amp;  &amp; T_{12} &amp;  &amp; T_{13} \\      T_{21} &amp;  &amp; T_{22} &amp;  &amp; T_{23} \\      T_{31} &amp;  &amp; T_{32} &amp;  &amp; T_{33}  \end{array} \right)  \]</span></p></li><li><p>Inner product or Dot product 内积 点积</p><p>In general， for engineering, inner and dot product can be treated the same operation</p><p>工科范围内，通常不怎么区分内积和点积</p><ul><li><p>The inner product of two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> produces a scalar <span class="math inline">\(\phi\)</span> and is commutative. This operation is indicated by the dot sign <span class="math inline">\(\bullet\)</span></p><p><span class="math display">\[\phi = \mathbf{a} \cdotp \mathbf{b} = \mathbf{a}^{T}  \mathbf{b} = \sum_{i=1}^3 a_i b_i\]</span></p></li><li><p>The inner product of vector <span class="math inline">\(\boldsymbol{a}\)</span> and a tensor <span class="math inline">\(\boldsymbol{T}\)</span> produces a vector <span class="math inline">\(\boldsymbol{b}\)</span>，is non-commutative if the tensor is non-symmetric:</p><p><span class="math display">\[ \mathbf{b} = \mathbf{T} \cdot \mathbf{a} = \sum_{i=1}^3 \sum_{j=1}^3 T_{ij}a_j = \left( \begin{array}{cccc}    T_{11} a_1 &amp; + &amp; T_{12} a_2 &amp; + &amp; T_{13} a_3 \\    T_{21} a_1 &amp; + &amp; T_{22} a_2 &amp; + &amp; T_{23} a_3 \\    T_{31} a_1 &amp; + &amp; T_{32} a_2 &amp; + &amp; T_{33} a_3\end{array} \right)\]</span></p><p><span class="math display">\[\mathbf{b} = \mathbf{a} \cdot \mathbf{T} = \sum_{i=1}^3 \sum_{j=1}^3 a_jT_{ji}\]</span></p><p>if <span class="math inline">\(\mathbf{T}\)</span> is a symmetric tensor, that is <span class="math inline">\(\mathbf{T}_{ij} = \mathbf{T}_{ji}\)</span>, then <span class="math inline">\(\mathbf{a} \cdot \mathbf{T} = \mathbf{T} \cdot \mathbf{a}\)</span></p></li></ul></li><li>Outer product 外积<ul><li><p>The outer product (dyadic product) of two vectors <span class="math inline">\(\mathbf{a}, \mathbf{b}\)</span> is a tensor</p><p><span class="math display">\[\mathbf{T}=\mathbf{a} \otimes \mathbf{b}=\mathbf{a b}^T=\left[\begin{array}{lll}a_x b_x &amp; a_x b_y &amp; a_x b_z \\a_y b_x &amp; a_y b_y &amp; a_y b_z \\a_z b_x &amp; a_z b_y &amp; a_z b_z\end{array}\right]\]</span></p><p>Usually, sign <span class="math inline">\(\otimes\)</span> is neglected for brevity as shown below:</p><p><span class="math display">\[\mathbf{a}\mathbf{b}\]</span></p><p>But the first one is mathematically correct.</p></li></ul></li><li>Cross product 叉乘<ul><li>the cross product of <span class="math inline">\(\mathbf{a} \times \mathbf{b}\)</span> is vector that perpendicular to both <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, which defines a plane.</li><li><p>the magnitude of the cross product of two vectors represents the area of the parallelogram spanned by the two vectors</p><p>两向量叉乘的模为该两向量所组成的平行四边形的面积</p></li></ul><p><span class="math display">\[\mathbf{a} \times \mathbf{b} = \left| \begin{array}{lll}                              \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\                              a_1 &amp; a_2 &amp; a_3\\                              b_1 &amp; b_2 &amp; b_3                              \end{array} \right|                             = \left[ \begin{array}{l}                             a_2 b_3 - b_2 a_3 \\                             a_1 b_3 - b_1 a_3 \\                             a_1 b_2 - b_1 a_2                             \end{array}\right]\]</span></p></li><li><p>Differential Operators</p><p>The spatial derivative of a variable (scalar, vector or tensor) is made by using the "Nabla" operator <span class="math inline">\(\nabla\)</span> or "del". It contains the three space derivatives of <span class="math inline">\(x,y,z\)</span> in a Cartesian coordinate system: <span class="math display">\[  \nabla = \left(\begin{array}{l}  \frac{\partial}{\partial x} \\  \frac{\partial}{\partial y} \\  \frac{\partial}{\partial z}  \end{array}\right)  \]</span></p><ul><li>Gradient Operator 梯度<ul><li><p>the gradient of a scalar <span class="math inline">\(\phi\)</span> results in a vector <span class="math inline">\(\mathbf{a}\)</span>: <span class="math display">\[  \text{grad} \phi = \nabla\phi = \left(\begin{array}{l}                                  \frac{\partial \phi}{\partial x} \\                                  \frac{\partial \phi}{\partial y} \\                                  \frac{\partial \phi}{\partial z}                                  \end{array}\right)  \]</span></p></li><li><p>the gradient of a vector <span class="math inline">\(\mathbf{b}\)</span> results in a tensor <span class="math inline">\(\mathbf{T}\)</span> <span class="math display">\[  \text{grad} \mathbf{b} = \nabla \otimes \mathbf{b} = \nabla \mathbf{b} = \left[\begin{array}{lll}  \frac{\partial}{\partial x} b_x &amp; \frac{\partial}{\partial x} b_y &amp; \frac{\partial}{\partial x} b_z \\  \frac{\partial}{\partial y} b_x &amp; \frac{\partial}{\partial y} b_y &amp; \frac{\partial}{\partial y} b_z \\  \frac{\partial}{\partial z} b_x &amp; \frac{\partial}{\partial z} b_y &amp; \frac{\partial}{\partial z} b_z  \end{array}\right]  \]</span> <u>So the gradient operation increases the rank of the tensor by one</u>.</p></li></ul></li><li>Divergence Operator 散度<ul><li>The divergence of a vector <span class="math inline">\(\mathbf{b}\)</span> results in a scalar <span class="math inline">\(\phi\)</span>, and can be expressed by the Nabla and the dot sign <span class="math inline">\(\bullet\)</span>, <span class="math inline">\(\nabla \bullet\)</span>: <span class="math display">\[  \text{div} \mathbf{b} = \nabla \bullet \mathbf{b} = \sum_{i=1}^3 \frac{\partial}{\partial x_i} \mathbf{b_i} = \frac{\partial b_1}{\partial x_1} + \frac{\partial b_2}{\partial x_2} + \frac{\partial b_3}{\partial x_3}  \]</span><ul><li>Physically the divergence of vector field over a region is a measure of how much the vector field points into or out of the region.</li></ul></li><li>The divergence of a tensor <span class="math inline">\(\mathbf{T}\)</span> results in a vector <span class="math inline">\(\mathbf{b}\)</span>: <span class="math display">\[  \text{div} \mathbf{T} = \nabla \bullet \mathbf{T} = \frac{\partial}{\partial x_j} \mathbf{T_{ji}} = \left[\begin{array}{llll}  \frac{\partial T_{11}}{\partial x_1} + \frac{\partial T_{21}}{\partial x_2} + \frac{\partial T_{31}}{\partial x_3} \\  \frac{\partial T_{12}}{\partial x_1} + \frac{\partial T_{22}}{\partial x_2} + \frac{\partial T_{32}}{\partial x_3} \\  \frac{\partial T_{13}}{\partial x_1} + \frac{\partial T_{23}}{\partial x_2} + \frac{\partial T_{33}}{\partial x_3}  \end{array}\right]  \]</span></li></ul><p><u>So the divergence operation decreases the rank of the tensor by one.</u></p><ul><li>The product Rule within the divergence operator<ul><li>The divergence of the product of vector <span class="math inline">\(\mathbf{a}\)</span> and a scalar <span class="math inline">\(\phi\)</span> <span class="math display">\[\nabla \bullet (\mathbf{a} \phi) = \mathbf{a} \bullet \nabla \phi + \phi \nabla \bullet \mathbf{a}\]</span></li><li>The divergence of the outer product of two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> <span class="math display">\[\nabla \bullet (\mathbf{a} \otimes \mathbf{b}) = \nabla \bullet (\mathbf{a} \mathbf{b}) = \mathbf{a} \bullet \nabla \mathbf{b} + \mathbf{b} \nabla \bullet \mathbf{a}\]</span></li></ul></li></ul></li><li>Other operations on the Nabla Operator<ul><li>the divergence of the gradient of a scalar variable <span class="math inline">\(\boldsymbol{s}\)</span> is denoted by the Laplacian of <span class="math inline">\(\boldsymbol{s}\)</span> and is a scalar</li></ul><p><span class="math display">\[ \nabla \cdot (\nabla\boldsymbol{s}) = \nabla ^2 \boldsymbol{s} = \frac{\partial ^2 \boldsymbol{s}}{\partial x^2} + \frac{\partial ^2 \boldsymbol{s}}{\partial y^2} + \frac{\partial ^2 \boldsymbol{s}}{\partial z^2}\]</span></p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;basic-mathematics&quot;&gt;Basic Mathematics&lt;/h1&gt;
&lt;h2 id=&quot;einsteins-summation-convention-爱因斯坦求和约定&quot;&gt;Einsteins Summation Convention (爱因斯坦求和约定)</summary>
      
    
    
    
    
  </entry>
  
</feed>
